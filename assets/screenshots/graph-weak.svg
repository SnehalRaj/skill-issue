<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 600 600" width="600" height="600">
  <defs>
    <linearGradient id="header-grad" x1="0%" y1="0%" x2="0%" y2="100%">
      <stop offset="0%" style="stop-color:#3d3d3d"/>
      <stop offset="100%" style="stop-color:#2d2d2d"/>
    </linearGradient>
  </defs>

  <!-- Window background -->
  <rect width="600" height="600" rx="8" fill="#1a1b26"/>

  <!-- Window header -->
  <rect width="600" height="40" rx="8" fill="url(#header-grad)"/>
  <rect y="32" width="600" height="8" fill="url(#header-grad)"/>

  <!-- Traffic lights -->
  <circle cx="20" cy="20" r="6" fill="#ff5f57"/>
  <circle cx="40" cy="20" r="6" fill="#febc2e"/>
  <circle cx="60" cy="20" r="6" fill="#28c840"/>

  <!-- Title -->
  <text x="300" y="25" text-anchor="middle" fill="#808080" font-family="SF Mono, Monaco, monospace" font-size="13">skill-issue graph weak --domain machine-learning</text>

  <!-- Terminal content -->
  <g font-family="SF Mono, Monaco, Consolas, monospace" font-size="13">
    <text x="20" y="75" fill="#c9d1d9">Top 5 Priority Nodes (machine-learning):</text>
    <text x="20" y="95" fill="#c9d1d9"></text>
    <text x="20" y="115" fill="#c9d1d9">  1. regularization</text>
    <text x="20" y="135" fill="#c9d1d9">     Regularization</text>
    <text x="20" y="155" fill="#c9d1d9">     Priority: 0.95 | Mastery: 0.00 | Status: weak</text>
    <text x="20" y="175" fill="#c9d1d9">     → What geometric effect does L1 regularization have on the weight vector?</text>
    <text x="20" y="195" fill="#c9d1d9"></text>
    <text x="20" y="215" fill="#c9d1d9">  2. cross-validation</text>
    <text x="20" y="235" fill="#c9d1d9">     Cross-Validation</text>
    <text x="20" y="255" fill="#c9d1d9">     Priority: 0.95 | Mastery: 0.00 | Status: weak</text>
    <text x="20" y="275" fill="#c9d1d9">     → Why is nested cross-validation necessary when tuning hyperparameters?</text>
    <text x="20" y="295" fill="#c9d1d9"></text>
    <text x="20" y="315" fill="#c9d1d9">  3. attention-mechanism</text>
    <text x="20" y="335" fill="#c9d1d9">     Attention &amp; Transformers</text>
    <text x="20" y="355" fill="#c9d1d9">     Priority: 0.95 | Mastery: 0.00 | Status: weak</text>
    <text x="20" y="375" fill="#c9d1d9">     → What is the time complexity of self-attention with respect to sequence length?</text>
    <text x="20" y="395" fill="#c9d1d9"></text>
    <text x="20" y="415" fill="#c9d1d9">  4. probability-distributions</text>
    <text x="20" y="435" fill="#c9d1d9">     Probability Distributions</text>
    <text x="20" y="455" fill="#c9d1d9">     Priority: 0.93 | Mastery: 0.00 | Status: weak</text>
    <text x="20" y="475" fill="#c9d1d9">     → Why does softmax output a probability distribution?</text>
    <text x="20" y="495" fill="#c9d1d9"></text>
    <text x="20" y="515" fill="#c9d1d9">  5. embeddings</text>
    <text x="20" y="535" fill="#c9d1d9">     Embeddings &amp; Representations</text>
    <text x="20" y="555" fill="#c9d1d9">     Priority: 0.90 | Mastery: 0.00 | Status: weak</text>
    <text x="20" y="575" fill="#c9d1d9">     → Why do word2vec embeddings capture semantic similarity?</text>
  </g>
</svg>