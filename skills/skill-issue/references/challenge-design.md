# Challenge Design Guide

## Principles of Good Challenge Design

### 1. Grounding
Every challenge MUST reference something the agent just did.
- âŒ Bad: "What's the complexity of quicksort?" (random)
- âœ… Good: "What's the complexity of the sorting function I just wrote for the pipeline?" (grounded)

### 2. Right-Sizing (30â€“90 seconds per challenge)
- **Pen & Paper**: Simple arithmetic, 2â€“3 step derivation, single formula application
- **Explain Back**: 2â€“3 sentences testing one core concept
- **Predict**: One specific, concrete output (a number, a state, a shape)
- **Spot the Bug**: One clear bug, possibly one subtle secondary issue
- **Complexity**: Big-O with brief justification
- **Connect the Dots**: 2â€“3 sentence connection between two specific things

### 3. Unambiguity
The correct answer must be clearly defined. For numerical answers, specify precision.
For conceptual answers, have 2â€“3 "must-mention" key concepts in mind before presenting.

### 4. Scaffolded Difficulty

**Apprentice**: Direct application of a single concept
- "What does this function return for input [5, 3, 8]?"
- "What's 3Ã—3 convolution output size on 28Ã—28 with no padding?"

**Practitioner**: Combining 2 concepts or multi-step reasoning
- "This function has O(nÂ²). How would you make it O(n log n)?"
- "Why does this ReLU activation cause dying neurons here?"

**Expert**: Deep understanding, edge case awareness, non-obvious connections
- "Why does the parameter shift rule need Ï€/2 and not some other value?"
- "When would this barren plateau mitigation strategy fail?"

**Master**: Novel synthesis, creative problem-solving, research-level insight
- "Can you derive a tighter bound on the expressibility of this circuit?"
- "How would you modify this shadow estimation protocol for non-Clifford measurements?"

---

## Challenge Type Examples

### Type 1: Pen & Paper ðŸ“
> ðŸ§  **SKILL CHECK #47** â€” `tensor-ops` â€” Difficulty: Practitioner
>
> I just wrote a function that applies a 3Ã—3 convolution with stride=2, padding=1
> to a 28Ã—28 single-channel input.
>
> â†’ Grab a pen. What's the spatial dimension of the output feature map?
>
> `[answer]` `[hint]` `[skip]`

**Evaluation**: Exact match for numerical answers. Show work if wrong.

---

### Type 2: Explain Back ðŸ—£ï¸
> ðŸ§  **SKILL CHECK #48** â€” `quantum-circuits` â€” Difficulty: Expert
>
> I just used the parameter shift rule to compute the gradient of an expectation value.
>
> â†’ In 2â€“3 sentences: Why is the shift exactly Ï€/2? What property of the gate generator
> makes this work?
>
> `[answer]` `[hint]` `[skip]`

**Key concepts to check for**: eigenvalue spectrum of generator, Â±1 eigenvalues of Pauli
generators â†’ Ï€/2 shift.

---

### Type 3: Predict the Output ðŸ”®
> ðŸ§  **SKILL CHECK #49** â€” `quantum-circuits` â€” Difficulty: Expert
>
> I built a 4-qubit circuit: RBS(Ï€/4) on qubits (0,1), then RBS(Ï€/3) on qubits (1,2).
> Input state: |1100âŸ©
>
> â†’ What is the probability of measuring qubit 2 in state |1âŸ©?
>
> `[answer]` `[hint]` `[skip]`

**Evaluation**: Accept Â±0.01 tolerance for probabilities.

---

### Type 4: Spot the Bug ðŸ›
> ðŸ§  **SKILL CHECK #50** â€” `python-debugging` â€” Difficulty: Practitioner
>
> Here's a version of the function I just wrote, but with a bug:
> ```python
> def softmax(x):
>     exp_x = np.exp(x)
>     return exp_x / np.sum(exp_x, axis=0)
> ```
> â†’ What's wrong, and when would it fail?
>
> `[answer]` `[hint]` `[skip]`

**Key bugs**: no max subtraction (numerical instability); `axis=0` wrong for batched inputs.

---

### Type 5: Complexity Check â±ï¸
> ðŸ§  **SKILL CHECK #51** â€” `algorithms` â€” Difficulty: Practitioner
>
> I just wrote a function that finds all pairs in an array summing to target K.
>
> â†’ What's the time and space complexity of my implementation?
> Bonus: Could it be done better?
>
> `[answer]` `[hint]` `[skip]`

**Evaluation**: Check Big-O notation. Bonus XP for optimization insights.

---

### Type 6: Connect the Dots ðŸ”—
> ðŸ§  **SKILL CHECK #52** â€” `quantum-ml` â€” Difficulty: Expert
>
> We just implemented classical shadow estimation for this circuit.
> Earlier this week, we worked on parameter shift gradients.
>
> â†’ What's the connection? Could classical shadows help with gradient estimation?
>
> `[answer]` `[hint]` `[skip]`

**Evaluation**: Open-ended. Award based on depth of insight.

---

## Topic-Specific Challenge Banks

### Quantum Computing
**pen-paper:** Circuit output states, RBS/Hamming weight proofs, expectation values for small
circuits, parameter shift derivations, gate decompositions.

**explain-back:** Why HHL doesn't give exponential speedup in practice; difference between
coherent and incoherent noise; barren plateau intuition; why VQE works variationaly.

**predict:** State after specific circuit sequence; measurement probability in subspace;
effect of noise channel on density matrix.

**spot-bug:** Wrong measurement basis; unnormalized state from encoding; gradient vanishes
every iteration in VQE; fidelity > 1 from indexing error.

**connect-dots:**
- Quantum Fisher information â†” Classical Fisher information / natural gradient
- Barren plateaus â†” Vanishing gradients / loss landscape sharpness
- Hamming weight preservation â†” Permutation equivariance in GNNs
- Classical shadows â†” Sketching algorithms in streaming
- QAOA â†” Simulated annealing / continuous relaxations
- Clifford circuits â†” Linear codes over GF(2)
- Tensor network contraction â†” Einsum in deep learning
- VQE â†” Expectation-maximization

---

### Machine Learning
**pen-paper:** Backprop for a single layer, cross-entropy as negative log-likelihood, Adam
update derivation, L2 reg as Gaussian prior.

**explain-back:** Why dropout works as regularization (ensemble interpretation); what attention
mechanism computes vs what CNNs do; difference between model capacity and generalization.

**predict:** Output shape through a series of layers; gradient flow in specific architecture;
what BatchNorm does differently at train vs eval.

**spot-bug:** NaN after epoch 1 (LR too high, missing clip); val acc stuck at 50% (data
leak / label shuffle); missing `model.eval()`.

**connect-dots:**
- Kernel trick â†” Feature maps in quantum computing
- ELBO in VAEs â†” Rate-distortion theory
- Skip connections â†” ODE perspective on neural networks
- LoRA â†” Low-rank approximation of weight updates

---

### Linear Algebra
Eigenvalue computation for 2Ã—2, 3Ã—3 matrices; matrix rank and null space; trace/determinant
properties; unitary/orthogonality verification; tensor products for small systems.

---

### Algorithms & Data Structures
Complexity analysis, correctness for edge cases, algorithm selection justification, recurrence
relations, amortized analysis.

---

### Python / Software Engineering
Mutability gotchas, generator vs list comprehension, decorator behavior, NumPy broadcasting
rules, concurrency pitfalls.
